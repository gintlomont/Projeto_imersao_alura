{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "O0sFK5BaVSE2"
      ],
      "mount_file_id": "1qjc3PaPBgaCIRZdPwrFGKmFVST-xUxG0",
      "authorship_tag": "ABX9TyNDbo30mm9r4MBxEclP2h9S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gintlomont/Projeto_imersao_alura/blob/main/projeto_Sistema_de_An%C3%A1lise_de_Dados_e_Conversa%C3%A7%C3%A3o_da_imersao_alura.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sistema de Análise de Dados e Conversação com Chat Utilizando Google Generative AI\n",
        "\n",
        "\n",
        "*   Este código cria um sistema no console que oferece análise de dados e chatbot. Com a API do Google Generative AI, os usuários podem carregar arquivos CSV, JSON ou TXT para análise, fornecendo prompts para orientar a análise.\n",
        "*   Além disso, podem conversar com um chatbot alimentado pela tecnologia de geração de linguagem natural do Google. Ideal para extrair insights de dados ou interagir com um chatbot avançado no console Python.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "Exemplos: <br>\n",
        "Voce pode usar o seu arquivo ou link da irmarsão (arquivo json)\n",
        "\n",
        "1.   https://raw.githubusercontent.com/guilhermeonrails/api-imersao-ia/main/words.json\n",
        "2.   Do que se trata este arquivo.\n",
        "3.   Descreve este arquivo.\n",
        "2.   Quais são os principais insights que podem ser extraídos deste dados.\n",
        "4.   Pergunte o que quiser sobre os seus arquivo ou dados\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WxPu_1Z_Ui8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.   Importar biblitecas\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "O0sFK5BaVSE2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gsAiBkS3S9sy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "\n",
        "# bibliteca que trabalha com formatação do texto\n",
        "import textwrap\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# bibliteca que limpa tela\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# bibloteca time\n",
        "from time import sleep"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Configurar a API do Google Generative AI"
      ],
      "metadata": {
        "id": "xbd_gz6YWY0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY = 'Coloque o se API_kEY'\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# configurar Genai\n",
        "generation_config = {\n",
        "  \"candidate_count\": 1,\n",
        "  \"temperature\": 1,\n",
        "}\n",
        "\n",
        "safety_settings={\n",
        "    'HATE': 'BLOCK_NONE',\n",
        "    'HARASSMENT': 'BLOCK_NONE',\n",
        "    'SEXUAL' : 'BLOCK_NONE',\n",
        "    'DANGEROUS' : 'BLOCK_NONE'\n",
        "    }\n",
        "model = genai.GenerativeModel('gemini-pro', safety_settings, generation_config, )"
      ],
      "metadata": {
        "id": "SzMut0RsWf6j"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Criando Funçoes importantes"
      ],
      "metadata": {
        "id": "MPiDbGvAXLde"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Analisador de dados"
      ],
      "metadata": {
        "id": "f5DjUKPEXWWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analisar_dados(dados, prompt=None):\n",
        "    \"\"\"Analisa os dados fornecidos usando o Gemini-pro e retorna os insights.\"\"\"\n",
        "\n",
        "    # Limitando a quantidade de linhas exibidas para evitar ultrapassar o numeros de tokens permitidos\n",
        "    #dados = dados[150].to_string()\n",
        "\n",
        "    # Preparar a entrada para o Gemini-pro\n",
        "    texto_entrada = f\"Dados: {dados}\\nPrompt: {prompt}\"\n",
        "\n",
        "    # Gerar insights com o Gemini-pro\n",
        "    resposta = chat.send_message(texto_entrada)\n",
        "    insights = resposta.text\n",
        "\n",
        "    # Retornar os insights\n",
        "    return insights"
      ],
      "metadata": {
        "id": "XXL2fDOYXtEK"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Detector de formato de dados"
      ],
      "metadata": {
        "id": "-16j4rhpX8EK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tem_formato(dados):\n",
        "  format = dados.split('.')\n",
        "  return format[-1]\n"
      ],
      "metadata": {
        "id": "2hLsjw6MYDWE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Tela de menu inicial, de exibição, titulo e de caregamento de dados"
      ],
      "metadata": {
        "id": "a0Pn8SX7biVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def menu_principal():\n",
        "    \"\"\"Exibe o menu principal e retorna a escolha do usuário.\"\"\"\n",
        "    titulo(\"Sistema de Análise de Dados e Conversação\")\n",
        "    print(\"  Selecione uma opção:\")\n",
        "    print(\"  1. Analisar os dados\")\n",
        "    print(\"  2. Conversar com o chat\")\n",
        "    print(\"  3. consultar o historico\\n\")\n",
        "    opcao = input(\"  Escolha uma opção (1 ou 2, 999 para sair.): \")\n",
        "    return opcao\n",
        "\n",
        "\n",
        "# função mostrar linhas de cabeçalho e centralizar o cabeçalho\n",
        "def titulo(msg):\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"{msg:^50}\")\n",
        "    print(\"-\" * 50, \"\\n\")\n",
        "\n",
        "\n",
        "def to_markdown(text):\n",
        "    text = text.replace('•', '  *')\n",
        "    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "\n",
        "# Função para carregar dados e extrair formato\n",
        "def carregar_dados():\n",
        "    caminho = input(\"Digite o caminho do arquivo: \")\n",
        "    formato_dados = tem_formato(caminho)  # verificar o formato de arquivo\n",
        "\n",
        "    # 1. Carregar e pré-processar os dados\n",
        "    if isinstance(formato_dados, str) and formato_dados in [\"csv\", \"json\", \"txt\"]:\n",
        "        try:\n",
        "            if formato_dados == \"csv\":\n",
        "                dados = pd.read_csv(caminho)\n",
        "            elif formato_dados == \"json\":\n",
        "                dados = pd.read_json(caminho)\n",
        "            elif formato_dados == \"txt\":\n",
        "                dados = pd.read_csv(caminho, delimiter='\\t')\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            return False, None\n",
        "    else:\n",
        "        # caso formato não seja [\"csv\", \"json\", \"txt\"]\n",
        "        return False, None\n",
        "    return True, dados\n"
      ],
      "metadata": {
        "id": "eIf3LoI2bpnt"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Conversar com Chat"
      ],
      "metadata": {
        "id": "ZueRLCR6b8d0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para conversar com o chat\n",
        "def conversar_chat():\n",
        "\n",
        "    # limpar tela ou console\n",
        "    clear_output()\n",
        "\n",
        "    # mostrar titulo\n",
        "    titulo(\"Chat de conversação\")\n",
        "\n",
        "    while True:\n",
        "\n",
        "        # receber chat de usuario\n",
        "        print(\"\\033[1;33mUser -> \\033[0m\")\n",
        "        prompt = input(\"Faça sua pergunta: \")\n",
        "        print()\n",
        "\n",
        "        # verificar a saida\n",
        "        if prompt == \"999\":\n",
        "            break\n",
        "\n",
        "        # fazer requisição e imprimir\n",
        "        resposta = chat.send_message(prompt)\n",
        "        print(\"\\033[1;32mResposta do chat:\\033[0m\")\n",
        "        display(to_markdown(f'\\n\\n{resposta.text}'))\n",
        "\n",
        "        print(\"\\n-----------------------------------------------------------\\n\")"
      ],
      "metadata": {
        "id": "3E4WHnuQcGm1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Consultar o historico da conversa"
      ],
      "metadata": {
        "id": "a08hLqULCZE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ver_historico():\n",
        "\n",
        "    # mostrar titulo\n",
        "    titulo(\"Historico de conversa\")\n",
        "\n",
        "    # Verificar se há conteúdo no histórico\n",
        "    if chat.history:\n",
        "        # Imprimir o histórico\n",
        "        for message in chat.history:\n",
        "            display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))\n",
        "            print('-------------------------------------------')\n",
        "    else:\n",
        "        print(\"O histórico está vazio.\")"
      ],
      "metadata": {
        "id": "6XNQFxE-C6fS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Programa principal"
      ],
      "metadata": {
        "id": "OWh2bMdscM-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop principal\n",
        "\n",
        "# fazendo requisição ao gemini no modo gerar historico\n",
        "chat = model.start_chat(history=[])\n",
        "while True:\n",
        "\n",
        "    # limpar tela ou console\n",
        "    clear_output()\n",
        "\n",
        "    opcao = menu_principal()\n",
        "\n",
        "    if opcao == '1':\n",
        "\n",
        "        # limpar tela ou console\n",
        "        clear_output()\n",
        "\n",
        "        # mostrar titulo\n",
        "        titulo(\"Sistema de Análise de Dados\")\n",
        "        foi_carregado, dados = carregar_dados()\n",
        "\n",
        "        if foi_carregado is True:\n",
        "\n",
        "            while True:\n",
        "\n",
        "                print(\"\\n\\033[1;33mUser -> \\033[0m\")\n",
        "                prompt = input(\"Digite uma pergunta sobre os dados: \")\n",
        "\n",
        "                # verificar a saida\n",
        "                if prompt == \"999\":\n",
        "                    break\n",
        "\n",
        "                # chamada de função responsavel por analisar dados\n",
        "                insights = analisar_dados(dados, prompt)\n",
        "\n",
        "                # imprimir o resultado\n",
        "                print()\n",
        "                print(\"\\033[1;32mResposta do chat:\\033[0m\")\n",
        "                display(to_markdown(f'\\n\\n{insights}'))\n",
        "\n",
        "                print(\"\\n-----------------------------------------------------------\\n\")\n",
        "\n",
        "        else:\n",
        "            #print(\"\\n\\033[1;31mERRO!\\033[m\")\n",
        "            print(\"\\n\\033[1;31mERRO!\\033[0m\")\n",
        "            print('Verifique se a extenão termina em (\"csv\", \"json\", \"txt\") ou se cominho esta correto...')\n",
        "            sleep(5)\n",
        "\n",
        "    elif opcao == '2':\n",
        "        # limpar tela ou console\n",
        "        clear_output()\n",
        "        resp = conversar_chat()\n",
        "\n",
        "    elif opcao == \"3\":\n",
        "        # limpar tela ou console\n",
        "        clear_output()\n",
        "        ver_historico()\n",
        "\n",
        "        # este comando so impede limpar tela ante que o usuario termina de ler historico\n",
        "        input(\"Digite qualquer letra para sair: \")\n",
        "\n",
        "    elif opcao == \"999\":\n",
        "        print(\"\\nSaindo...\")\n",
        "        sleep(1)\n",
        "        print(\"Obrigado por usar o nosso sistema. Volte sempre e até logo 👋😊.\")\n",
        "        break\n",
        "\n",
        "    else:\n",
        "        print(\"  \\033[1;31mOpção inválida. Por favor, escolha 1, 2 ou 3.\\033[0m\")\n",
        "        sleep(2)\n",
        ""
      ],
      "metadata": {
        "id": "tKWDv29mcTIA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}